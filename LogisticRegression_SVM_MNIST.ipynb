{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training set Accuracy:84.908%\n",
      "[[4832    1   15    8   10   18   25    7    2    5]\n",
      " [   2 5649   33   12    3   21    4   11    0    7]\n",
      " [  36   42 4590   70   53   27   55   68    1   16]\n",
      " [  21   26  134 4658    9  149   21   45    1   67]\n",
      " [   9   20   24    5 4569   13   27   12    0  163]\n",
      " [  47   18   34  131   47 3966   89   19    8   62]\n",
      " [  26   12   28    2   19   75 4746    4    3    3]\n",
      " [  12   22   50   11   44   11    3 4972    0  140]\n",
      " [ 136  293  840 1004  193 1317  133   56   39  840]\n",
      " [  26   22   14   88  164   45    1  156    0 4433]]\n",
      "\n",
      " Validation set Accuracy:83.74%\n",
      "[[979   0   1   3   1   8   6   1   0   1]\n",
      " [  0 979   4   4   2   8   0   1   0   2]\n",
      " [ 11  18 894  22  13   5  14  15   1   7]\n",
      " [  4   9  31 892   4  26   4  13   0  17]\n",
      " [  1   5   7   2 942   3   7   0   0  33]\n",
      " [  9   9   8  40  19 886  17   2   0  10]\n",
      " [  7   3   7   0   6  15 959   2   0   1]\n",
      " [  3   4   9   1  15   0   0 926   0  42]\n",
      " [ 32  76 194 224  33 250  42   6  11 132]\n",
      " [ 10   3   5  20  23   5   1  27   0 906]]\n",
      "\n",
      " Testing set Accuracy:84.18%\n",
      "[[ 962    0    1    2    1    4    5    4    0    1]\n",
      " [   0 1123    4    1    0    2    4    1    0    0]\n",
      " [   9   12  943   21   12    4   14   13    0    4]\n",
      " [   4    1   21  929    2   23    4   13    0   13]\n",
      " [   1    2    6    3  917    0    9    2    1   41]\n",
      " [  11    4    5   37   14  783   20   10    0    8]\n",
      " [   9    4    8    2    4   20  910    1    0    0]\n",
      " [   2    9   22    5    8    2    1  951    0   28]\n",
      " [  34   42  152  199   58  259   43   20    3  164]\n",
      " [  10    8    1   18   36   13    1   25    0  897]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" \n",
    "     Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "    \"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    n_feature = mat.get(\"train1\").shape[1]\n",
    "    n_sample = 0\n",
    "    for i in range(10):\n",
    "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
    "    n_validation = 1000\n",
    "    n_train = n_sample - 10 * n_validation\n",
    "\n",
    "    # Construct validation data\n",
    "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
    "    for i in range(10):\n",
    "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
    "\n",
    "    # Construct validation label\n",
    "    validation_label = np.ones((10 * n_validation, 1))\n",
    "    for i in range(10):\n",
    "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
    "\n",
    "    # Construct training data and label\n",
    "    train_data = np.zeros((n_train, n_feature))\n",
    "    train_label = np.zeros((n_train, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
    "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
    "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
    "        temp = temp + size_i - n_validation\n",
    "\n",
    "    # Construct test data and label\n",
    "    n_test = 0\n",
    "    for i in range(10):\n",
    "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
    "    test_data = np.zeros((n_test, n_feature))\n",
    "    test_label = np.zeros((n_test, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
    "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
    "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
    "        temp = temp + size_i\n",
    "\n",
    "    # Delete features which don't provide any useful information for classifiers\n",
    "    sigma = np.std(train_data, axis=0)\n",
    "    index = np.array([])\n",
    "    for i in range(n_feature):\n",
    "        if (sigma[i] > 0.001):\n",
    "            index = np.append(index, [i])\n",
    "    train_data = train_data[:, index.astype(int)]\n",
    "    validation_data = validation_data[:, index.astype(int)]\n",
    "    test_data = test_data[:, index.astype(int)]\n",
    "\n",
    "    # Scale data to 0 and 1\n",
    "    train_data /= 255.0\n",
    "    validation_data /= 255.0\n",
    "    test_data /= 255.0\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def blrObjFunction(initialWeights, *args):\n",
    "    \"\"\"\n",
    "    blrObjFunction computes 2-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector (w_k) of size (D + 1) x 1 \n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector (y_k) of size N x 1 where each entry can be either 0 or 1 representing the label of corresponding feature vector\n",
    "\n",
    "    Output: \n",
    "        error: the scalar value of error function of 2-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 1 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, labeli = args\n",
    "\n",
    "    n_data = train_data.shape[0]\n",
    "    n_features = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_features + 1, 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "\n",
    "    \n",
    "    initialWeights = initialWeights.reshape((n_feature+1,1))     # add new column for Bias\n",
    "    initWeight = np.ones((n_data,1))    # add 1s to signify bias\n",
    "    dataWithBias = np.hstack((initWeight,train_data))    # Add Bias to dataset\n",
    "    op = sigmoid(np.dot(dataWithBias,initialWeights))    # Taking the sigmoid \n",
    "    error = np.sum((labeli * np.log(op)) + (1.0 - labeli) * np.log(1.0 - op))    \n",
    "    error = (-1/n_data)*error     #Calculating error normalized\n",
    "    error_grad = (op - labeli) * dataWithBias\n",
    "    error_grad = np.sum(error_grad, axis=0)\n",
    "    error_grad = error_grad/n_data    #Calculating error gradient normalized\n",
    "    \n",
    "    return error, error_grad\n",
    "\n",
    "\n",
    "def blrPredict(W, data):\n",
    "    \"\"\"\n",
    "     blrObjFunction predicts the label of data given the data and parameter W \n",
    "     of Logistic Regression\n",
    "     \n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight \n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "         \n",
    "     Output: \n",
    "         label: vector of size N x 1 representing the predicted label of \n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    \n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "    dataWithBias = np.hstack((np.ones((data.shape[0], 1)),data))    #adding bias\n",
    "    output = sigmoid(np.dot(dataWithBias,W))\n",
    "    label = np.argmax(output,axis=1)\n",
    "    label = label.reshape((data.shape[0],1))\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Script for Logistic Regression\n",
    "\"\"\"\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "# number of classes\n",
    "n_class = 10\n",
    "\n",
    "# number of training samples\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "# number of features\n",
    "n_feature = train_data.shape[1]\n",
    "\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "\n",
    "# Logistic Regression with Gradient Descent\n",
    "W = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights = np.zeros((n_feature + 1, 1))\n",
    "opts = {'maxiter': 100}\n",
    "for i in range(n_class):\n",
    "    labeli = Y[:, i].reshape(n_train, 1)\n",
    "    args = (train_data, labeli)\n",
    "    nn_params = minimize(blrObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
    "    W[:, i] = nn_params.x.reshape((n_feature + 1,))\n",
    "\n",
    "# Find the accuracy on Training Dataset\n",
    "predicted_label = blrPredict(W, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')\n",
    "cm = metrics.confusion_matrix(train_label, predicted_label)\n",
    "print(cm)\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label = blrPredict(W, validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%')\n",
    "cm = metrics.confusion_matrix(validation_label, predicted_label)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label = blrPredict(W, test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')\n",
    "cm = metrics.confusion_matrix(test_label, predicted_label)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------SVM-------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script for Support Vector Machine\n",
    "\"\"\"\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "print('\\n\\n--------------SVM-------------------\\n\\n')\n",
    "##################\n",
    "# YOUR CODE HERE #\n",
    "##################\n",
    "\n",
    "SVM_linear = SVC(kernel = 'linear')\n",
    "SVM_linear.fit(train_data,train_label.flatten())\n",
    "print('training  ' + str(100 * SVM_linear.score(train_data,train_label)))\n",
    "print('validation ' + str(100 * SVM_linear.score(validation_data,validation_label)))\n",
    "print('testing   ' + str(100 * SVM_linear.score(test_data,test_label)))\n",
    "\n",
    "SVM_RBF_1 = SVC(kernel = 'rbf', gamma = 1.0)\n",
    "SVM_RBF_1.fit(train_data,train_label.flatten())\n",
    "print('training  ' + str(100 * SVM_RBF_1.score(train_data,train_label)))\n",
    "print('validation  ' + str(100 * SVM_RBF_1.score(validation_data,validation_label)))\n",
    "print('testing  ' + str(100 * SVM_RBF_1.score(test_data,test_label)))\n",
    "\n",
    "SVM_RBF_AUTO = SVC(kernel = 'rbf', gamma = 'auto')\n",
    "SVM_RBF_AUTO.fit(train_data,train_label.flatten())\n",
    "print('training  ' + str(100 * SVM_RBF_AUTO.score(train_data,train_label)))\n",
    "print('validation  ' + str(100 * SVM_RBF_AUTO.score(validation_data,validation_label)))\n",
    "print('testing  ' + str(100 * SVM_RBF_AUTO.score(test_data,test_label)))\n",
    "\n",
    "c_vals = np.array([1,10,20,30,40,50,60,70,80,90,100])\n",
    "train_accuracy = np.zeros(11)\n",
    "valid_accuracy = np.zeros(11)\n",
    "test_accuracy = np.zeros(11)\n",
    "\n",
    "\n",
    "for i in range(len(c_vals)):   \n",
    "    x = SVC(c_vals[i],kernel='rbf')\n",
    "    x.fit(train_data, train_label.flatten())\n",
    "    train_accuracy[i] = 100*x.score(train_data, train_label)\n",
    "    valid_accuracy[i] = 100*x.score(validation_data, validation_label)\n",
    "    test_accuracy[i] = 100*x.score(test_data, test_label)\n",
    "    print('Train Accuracy for: ',c_vals[i],' -> ',train_accuracy[i])\n",
    "    print('Validation Accuracy for: ',c_vals[i],' -> ',valid_accuracy[i])\n",
    "    print('Test Accuracy for: ',c_vals[i],' -> ',test_accuracy[i])\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(c_vals,train_accuracy,'b',label='Training Accuracy',linewidth=3)\n",
    "plt.plot(c_vals,valid_accuracy,'g',label='Validation Accuracy',linewidth=3)\n",
    "plt.plot(c_vals,test_accuracy,'y',label='Test Accuracy',linewidth=3)\n",
    "\n",
    "plt.title('Variation of Accuracies with C Values')\n",
    "plt.ylabel('Accuracies')\n",
    "plt.xlabel('C values')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True,color='k')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('myfig')\n",
    "plt.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training set Accuracy:93.448%\n",
      "[[4786    1   12    7   11   33   30    7   32    4]\n",
      " [   1 5592   26   17    6   19    2   13   58    8]\n",
      " [  23   45 4503   72   58   24   59   53  108   13]\n",
      " [  14   18   95 4654    4  148   15   39  105   39]\n",
      " [   8   20   21    7 4576    6   42   13   24  125]\n",
      " [  39   13   36  117   34 3963   68   18  102   31]\n",
      " [  23   11   29    1   24   52 4758    2   16    2]\n",
      " [   8   16   49   18   34    9    4 4989   14  124]\n",
      " [  22   75   51  103   16  113   23   16 4387   45]\n",
      " [  17   18    9   55  126   30    2  134   42 4516]]\n",
      "\n",
      " Validation set Accuracy:92.48%\n",
      "[[975   0   1   3   2   7   3   2   6   1]\n",
      " [  0 972   3   2   1   5   0   2  13   2]\n",
      " [ 10  13 896  22  13   4  11   9  18   4]\n",
      " [  1   7  23 902   3  28   2  12  13   9]\n",
      " [  1   4   8   3 941   1  10   2   7  23]\n",
      " [  9   4   6  37  17 884  14   2  22   5]\n",
      " [  9   2   4   1   7  12 957   1   6   1]\n",
      " [  2   3   9   0   9   1   0 931   3  42]\n",
      " [ 13  17  19  27   9  20  19   2 868   6]\n",
      " [  4   3   5  14  19   4   1  24   4 922]]\n",
      "\n",
      " Testing set Accuracy:92.55%\n",
      "[[ 960    0    0    3    0    6    6    4    1    0]\n",
      " [   0 1110    3    2    0    2    4    2   12    0]\n",
      " [   6    8  924   16   10    3   14    8   39    4]\n",
      " [   4    1   20  914    0   25    3   10   26    7]\n",
      " [   1    1    6    2  921    0    9    4    9   29]\n",
      " [  10    2    2   37   10  773   15    6   30    7]\n",
      " [   9    3    4    2    7   15  914    3    1    0]\n",
      " [   1    9   19    6    6    2    0  952    2   31]\n",
      " [   9    8    6   26    9   23   10    8  868    7]\n",
      " [  11    8    0   10   28    5    0   20    8  919]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" \n",
    "     Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "    \"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    n_feature = mat.get(\"train1\").shape[1]\n",
    "    n_sample = 0\n",
    "    for i in range(10):\n",
    "        n_sample = n_sample + mat.get(\"train\" + str(i)).shape[0]\n",
    "    n_validation = 1000\n",
    "    n_train = n_sample - 10 * n_validation\n",
    "\n",
    "    # Construct validation data\n",
    "    validation_data = np.zeros((10 * n_validation, n_feature))\n",
    "    for i in range(10):\n",
    "        validation_data[i * n_validation:(i + 1) * n_validation, :] = mat.get(\"train\" + str(i))[0:n_validation, :]\n",
    "\n",
    "    # Construct validation label\n",
    "    validation_label = np.ones((10 * n_validation, 1))\n",
    "    for i in range(10):\n",
    "        validation_label[i * n_validation:(i + 1) * n_validation, :] = i * np.ones((n_validation, 1))\n",
    "\n",
    "    # Construct training data and label\n",
    "    train_data = np.zeros((n_train, n_feature))\n",
    "    train_label = np.zeros((n_train, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"train\" + str(i)).shape[0]\n",
    "        train_data[temp:temp + size_i - n_validation, :] = mat.get(\"train\" + str(i))[n_validation:size_i, :]\n",
    "        train_label[temp:temp + size_i - n_validation, :] = i * np.ones((size_i - n_validation, 1))\n",
    "        temp = temp + size_i - n_validation\n",
    "\n",
    "    # Construct test data and label\n",
    "    n_test = 0\n",
    "    for i in range(10):\n",
    "        n_test = n_test + mat.get(\"test\" + str(i)).shape[0]\n",
    "    test_data = np.zeros((n_test, n_feature))\n",
    "    test_label = np.zeros((n_test, 1))\n",
    "    temp = 0\n",
    "    for i in range(10):\n",
    "        size_i = mat.get(\"test\" + str(i)).shape[0]\n",
    "        test_data[temp:temp + size_i, :] = mat.get(\"test\" + str(i))\n",
    "        test_label[temp:temp + size_i, :] = i * np.ones((size_i, 1))\n",
    "        temp = temp + size_i\n",
    "\n",
    "    # Delete features which don't provide any useful information for classifiers\n",
    "    sigma = np.std(train_data, axis=0)\n",
    "    index = np.array([])\n",
    "    for i in range(n_feature):\n",
    "        if (sigma[i] > 0.001):\n",
    "            index = np.append(index, [i])\n",
    "    train_data = train_data[:, index.astype(int)]\n",
    "    validation_data = validation_data[:, index.astype(int)]\n",
    "    test_data = test_data[:, index.astype(int)]\n",
    "\n",
    "    # Scale data to 0 and 1\n",
    "    train_data /= 255.0\n",
    "    validation_data /= 255.0\n",
    "    test_data /= 255.0\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n",
    "\n",
    "\n",
    "\n",
    "def mlrObjFunction(params, *args):\n",
    "    \"\"\"\n",
    "    mlrObjFunction computes multi-class Logistic Regression error function and\n",
    "    its gradient.\n",
    "\n",
    "    Input:\n",
    "        initialWeights: the weight vector of size (D + 1) x 1\n",
    "        train_data: the data matrix of size N x D\n",
    "        labeli: the label vector of size N x 1 where each entry can be either 0 or 1\n",
    "                representing the label of corresponding feature vector\n",
    "\n",
    "    Output:\n",
    "        error: the scalar value of error function of multi-class logistic regression\n",
    "        error_grad: the vector of size (D+1) x 10 representing the gradient of\n",
    "                    error function\n",
    "    \"\"\"\n",
    "    train_data, labeli = args\n",
    "    n_data = train_data.shape[0]\n",
    "    n_feature = train_data.shape[1]\n",
    "    error = 0\n",
    "    error_grad = np.zeros((n_feature + 1, n_class))\n",
    "    \n",
    "    \n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    \n",
    "        \n",
    "    \n",
    "    w = params.reshape(n_feature+1,n_class)   # adding the bias column\n",
    "    wVals = np.ones((n_data,1))     #adding 1s for bias value\n",
    "    dataWithBias = np.hstack((wVals,train_data))     # adding the bias to data\n",
    "    \n",
    "\n",
    "    theta = np.dot(dataWithBias,w)\n",
    "    theta = np.exp(theta)\n",
    "    den = np.sum(theta,1)\n",
    "\n",
    "    den = np.reshape(den,(n_data,1))\n",
    "    theta = theta/den\n",
    "       \n",
    "    error = -1 * np.sum(np.sum((labeli * np.log(theta))))\n",
    "    error = error/n_data\n",
    "    \n",
    "    \n",
    "    error_grad_matrix = np.dot(dataWithBias.T, (theta - labeli))\n",
    "    error_grad = error_grad_matrix.flatten()\n",
    "    error_grad = error_grad/n_data\n",
    "    \n",
    "    #print(error)\n",
    "    #print(np.linalg.norm(error_grad))\n",
    "\n",
    "    \n",
    "    return error, error_grad\n",
    "\n",
    "\n",
    "def mlrPredict(W, data):\n",
    "    \"\"\"\n",
    "     mlrObjFunction predicts the label of data given the data and parameter W\n",
    "     of Logistic Regression\n",
    "\n",
    "     Input:\n",
    "         W: the matrix of weight of size (D + 1) x 10. Each column is the weight\n",
    "         vector of a Logistic Regression classifier.\n",
    "         X: the data matrix of size N x D\n",
    "\n",
    "     Output:\n",
    "         label: vector of size N x 1 representing the predicted label of\n",
    "         corresponding feature vector given in data matrix\n",
    "\n",
    "    \"\"\"\n",
    "    label = np.zeros((data.shape[0], 1))\n",
    "    n_data = data.shape[0]\n",
    "\n",
    "    ##################\n",
    "    # YOUR CODE HERE #\n",
    "    ##################\n",
    "    # HINT: Do not forget to add the bias term to your input data\n",
    "    \n",
    "    dataWithBias = np.hstack((np.ones((n_data,1)),data))    #adding the bias column to data\n",
    "    \n",
    "    theta = np.dot(dataWithBias,W)\n",
    "    theta = np.exp(theta)\n",
    "    den = np.sum(theta,1)\n",
    "    den = np.reshape(den,(n_data,1))\n",
    "    \n",
    "    theta = theta/den\n",
    "    \n",
    "    for i in range(theta.shape[0]):\n",
    "        label[i] = np.argmax(theta[i])\n",
    "    label = label.reshape(label.shape[0], 1)\n",
    "    \n",
    "    return label\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Script for Extra Credit Part\n",
    "\"\"\"\n",
    "\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "n_feature = train_data.shape[1]\n",
    "\n",
    "# number of classes\n",
    "n_class = 10\n",
    "\n",
    "# number of training samples\n",
    "n_train = train_data.shape[0]\n",
    "\n",
    "Y = np.zeros((n_train, n_class))\n",
    "for i in range(n_class):\n",
    "    Y[:, i] = (train_label == i).astype(int).ravel()\n",
    "\n",
    "\n",
    "# FOR EXTRA CREDIT ONLY\n",
    "W_b = np.zeros((n_feature + 1, n_class))\n",
    "initialWeights_b = np.zeros((n_feature + 1, n_class))\n",
    "opts_b = {'maxiter': 100}\n",
    "\n",
    "args_b = (train_data, Y)\n",
    "nn_params = minimize(mlrObjFunction, initialWeights_b, jac=True, args=args_b, method='CG', options=opts_b)\n",
    "W_b = nn_params.x.reshape((n_feature + 1, n_class))\n",
    "\n",
    "# Find the accuracy on Training Dataset\n",
    "predicted_label_b = mlrPredict(W_b, train_data)\n",
    "print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label_b == train_label).astype(float))) + '%')\n",
    "cm = metrics.confusion_matrix(train_label, predicted_label_b)\n",
    "print(cm)\n",
    "\n",
    "# Find the accuracy on Validation Dataset\n",
    "predicted_label_b = mlrPredict(W_b, validation_data)\n",
    "print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label_b == validation_label).astype(float))) + '%')\n",
    "cm = metrics.confusion_matrix(validation_label, predicted_label_b)\n",
    "print(cm)\n",
    "\n",
    "# Find the accuracy on Testing Dataset\n",
    "predicted_label_b = mlrPredict(W_b, test_data)\n",
    "print('\\n Testing set Accuracy:' + str(100 * np.mean((predicted_label_b == test_label).astype(float))) + '%')\n",
    "cm = metrics.confusion_matrix(test_label, predicted_label_b)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
